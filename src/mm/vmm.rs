//! # Virtual Memory Manager (VMM)
//!
//! O `VMM` implementa a pagina√ß√£o de 4 N√≠veis (x86_64 PML4) e gerencia o espa√ßo de endere√ßamento virtual do kernel.
//!
//! ## üéØ Prop√≥sito e Responsabilidade
//! - **Page Table Management:** Cria, modifica e navega na hierarquia PML4 ‚Üí PDPT ‚Üí PD ‚Üí PT.
//! - **Memory Mapping:** Mapeia endere√ßos f√≠sicos arbitr√°rios em virtuais (`map_page`).
//! - **Fine-Grained Access:** Divide "Huge Pages" (2MiB) em 512 p√°ginas de 4KiB sob demanda para permitir prote√ß√£o granular.
//!
//! ## üèóÔ∏è Arquitetura Singular: Scratch Slot & Huge Splitting
//! Diferente de VMMs acad√™micos, este VMM resolve problemas reais de hardware moderno:
//!
//! 1. **Scratch Slot:** Uma regi√£o virtual fixa (`0xFFFF_FE00_...`) usada para mapear temporariamente frames f√≠sicos.
//!    - *Por que?* Para zerar uma nova Page Table antes de inseri-la na hierarquia, sem depender de "Identity Map" (que pode n√£o cobrir toda a RAM).
//! 2. **Auto-Splitting:** Se `map_page` encontra uma Huge Page (2MB) no caminho, ele a converte atomicamente em uma tabela de p√°ginas menores.
//!    - *Por que?* Bootloaders mapeiam 0-4GB como Huge Pages para performance. O kernel precisa de granularidade 4KB para `MPROTECT` e `Guard Pages`.
//!
//! ## üîç An√°lise Cr√≠tica (Kernel Engineer's View)
//!
//! ### ‚úÖ Pontos Fortes
//! - **Isolamento de Boot:** O uso do Scratch Slot desacopla a inicializa√ß√£o do VMM das decis√µes do bootloader.
//! - **Robustez:** A l√≥gica de *Splitting* permite que o kernel refine permiss√µes de mem√≥ria (ex: tornar `.rodata` Read-Only) mesmo se o bootloader entregou tudo como RWX Huge Pages.
//!
//! ### ‚ö†Ô∏è Pontos de Aten√ß√£o (D√≠vida T√©cnica)
//! - **TLB Shootdown Inexistente:** Em multicore, alterar uma page table aqui **n√£o** invalida o TLB de outros CPUs.
//!   - *Consequ√™ncia:* Risco grav√≠ssimo de corrup√ß√£o de mem√≥ria em SMP.
//! - **Aus√™ncia de `unmap`:** Atualmente o VMM s√≥ sabe mapear. N√£o h√° l√≥gica para remover mapeamentos e liberar frames das page tables intermedi√°rias.
//! - **Hardcoded Offsets:** Os √≠ndices PML4 (Kernel, Heap, Scratch) s√£o constantes m√°gicas que devem bater com o `Ignite`. Desalinhamento = Crash.
//!
//! ## üõ†Ô∏è TODOs e Roadmap
//! - [ ] **TODO: (Critical/SMP)** Implementar **TLB Shootdown**.
//!   - *A√ß√£o:* Enviar IPI (Inter-Processor Interrupt) para todos os cores executarem `invlpg` ao alterar mapeamentos globais.
//! - [ ] **TODO: (Management)** Implementar `unmap_page(virt_addr)`.
//!   - *Requisito:* Liberar frames f√≠sicos se a Page Table ficar vazia (Reclaim).
//! - [ ] **TODO: (Security)** Implementar bits **NX (No-Execute)** e **USER/SUPERVISOR** rigorosos.
//!   - *Alvo:* Garantir que Heap/Stack n√£o sejam execut√°veis (W^X).
//! - [ ] **TODO: (Feature)** Suporte a **5-Level Paging** (Ice Lake+).
//!   - *Impacto:* Permitir endere√ßamento virtual acima de 256 TiB (futuro distante).
//!
//! ----------------------------------------------------------------------
//! ARQUITETURA DE PAGINA√á√ÉO x86_64
//! ----------------------------------------------------------------------
//!
//! Endere√ßo virtual can√¥nico (48 bits significativos):
//!
//! ```text
//! ‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚î¨‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚î¨‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚î¨‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚î¨‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê
//! ‚îÇ PML4 Index ‚îÇ PDPT Index ‚îÇ  PD Index  ‚îÇ  PT Index  ‚îÇ   Offset   ‚îÇ
//! ‚îÇ  (9 bits)  ‚îÇ  (9 bits)  ‚îÇ  (9 bits)  ‚îÇ  (9 bits)  ‚îÇ  (12 bits) ‚îÇ
//! ‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚î¥‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚î¥‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚î¥‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚î¥‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò
//!      bits 47-39    38-30       29-21        20-12         11-0
//! ```
//!
//! Cada n√≠vel cont√©m 512 entradas (512 √ó 8 bytes = 4 KiB).
//! A tradu√ß√£o √© resolvida do n√≠vel mais alto (PML4) at√© o PT,
//! exceto quando huge pages est√£o presentes.
//!
//! ----------------------------------------------------------------------
//! PROBLEMA CL√ÅSSICO: HUGE PAGES vs PAGE TABLES (4 KiB)
//! ----------------------------------------------------------------------
//!
//! O bootloader cria um **identity map de 0‚Äì4 GiB usando huge pages (2 MiB)**.
//! Isso reduz o n√∫mero de page tables e acelera o boot.
//!
//! ‚ö†Ô∏è PROBLEMA CR√çTICO:
//! Quando uma entrada de PD possui a flag `PAGE_HUGE`:
//!
//! - o CPU **ignora completamente** o n√≠vel PT;
//! - qualquer tentativa de criar page tables de 4 KiB dentro dessa regi√£o
//!   √© ignorada;
//! - acessos resultam em **General Protection Fault (GPF)**.
//!
//! Esse comportamento causava crashes ao:
//! - alocar page tables dinamicamente;
//! - zerar frames f√≠sicos rec√©m-alocados;
//! - mapear regi√µes internas ao identity map.
//!
//! ----------------------------------------------------------------------
//! SOLU√á√ÉO ARQUITETURAL ADOTADA
//! ----------------------------------------------------------------------
//!
//! A solu√ß√£o combina **isolamento** + **adapta√ß√£o din√¢mica**:
//!
//! 1. SCRATCH SLOT
//!    - Regi√£o virtual fixa, isolada e garantidamente N√ÉO huge;
//!    - usada exclusivamente para mapear temporariamente frames f√≠sicos;
//!    - permite zerar frames antes de inseri-los na hierarquia de page tables;
//!    - elimina o problema "chicken-and-egg" da inicializa√ß√£o.
//!
//! 2. SPLIT AUTOM√ÅTICO DE HUGE PAGES
//!    - ao detectar uma entrada com `PAGE_HUGE`:
//!      - uma nova Page Table (PT) √© alocada;
//!      - o mapeamento da huge page (2 MiB) √© replicado em
//!        512 p√°ginas de 4 KiB;
//!      - a entrada do PD √© substitu√≠da por um ponteiro para a nova PT;
//!      - o comportamento funcional original √© preservado.
//!
//! Resultado:
//! - o kernel pode criar page tables 4 KiB em qualquer regi√£o;
//! - nenhuma depend√™ncia de identity map para opera√ß√µes internas;
//! - elimina√ß√£o definitiva de GPFs causados por huge pages.
//!
//! ----------------------------------------------------------------------
//! REGI√ïES IMPORTANTES DO ESPA√áO VIRTUAL
//! ----------------------------------------------------------------------
//!
//! Conven√ß√£o atual de layout:
//!
//! - PML4[0..3]   ‚Üí Identity map (0‚Äì4 GiB, huge pages)
//! - PML4[288]    ‚Üí Heap do kernel
//! - PML4[511]    ‚Üí Higher-half do kernel
//! - PML4[508]    ‚Üí SCRATCH SLOT (isolado, limpo, seguro)
//!
//! ‚ö†Ô∏è Qualquer altera√ß√£o nesses √≠ndices **DEVE** ser refletida
//! no bootloader (Ignite). Desalinhamento aqui causa falhas
//! dif√≠ceis de diagnosticar.
//!
//! ----------------------------------------------------------------------
//! CONTRATOS E INVARIANTES (N√ÉO QUEBRE)
//! ----------------------------------------------------------------------
//!
//! 1. O bootloader deve:
//!    - fornecer identity map 0‚Äì4 GiB usando huge pages;
//!    - reservar corretamente o scratch slot no PML4.
//!
//! 2. `init()` deve ser chamada:
//!    - exatamente uma vez;
//!    - em early-boot;
//!    - antes de qualquer uso do heap.
//!
//! 3. O PMM (Physical Memory Manager) deve estar operacional
//!    antes de qualquer tentativa de criar page tables.
//!
//! 4. Toda nova page table alocada √©:
//!    - zerada antes do uso;
//!    - publicada na hierarquia apenas ap√≥s estar consistente.
//!
//! ----------------------------------------------------------------------
//! SEGURAN√áA E USO DE `unsafe`
//! ----------------------------------------------------------------------
//!
//! Este m√≥dulo utiliza `unsafe` por necessidade arquitetural:
//! - escrita direta em page tables;
//! - convers√£o de endere√ßos f√≠sicos em ponteiros;
//! - manipula√ß√£o expl√≠cita de CR3 e TLB.
//!
//! Medidas adotadas:
//! - `invlpg` ap√≥s modifica√ß√µes relevantes;
//! - zeragem expl√≠cita de frames;
//! - isolamento do scratch slot.
//!
//! ----------------------------------------------------------------------
//! RISCOS CONHECIDOS
//! ----------------------------------------------------------------------
//!
//! - Scratch slot ausente ou mal configurado pode ativar fallback
//!   inseguro (escrita direta via identity map);
//! - Split de huge page pode falhar em condi√ß√µes severas de OOM;
//! - Em SMP, este c√≥digo assume execu√ß√£o single-core
//!   (n√£o h√° TLB shootdown).
//!
//! ----------------------------------------------------------------------
//! EXTENS√ïES FUTURAS RECOMENDADAS
//! ----------------------------------------------------------------------
//!
//! - Migrar retornos `bool` / `0` para `Result<T, Error>`;
//! - Implementar `unmap_page()` e valida√ß√µes de mapeamento;
//! - Rollback seguro em split parcial de huge page;
//! - Protocolo de TLB shootdown para SMP.
//!
//! ----------------------------------------------------------------------
//! Abaixo deste ponto: implementa√ß√£o.
//! Coment√°rios locais explicam decis√µes espec√≠ficas.
use crate::mm::addr::phys_to_virt;
use crate::mm::error::{MmError, MmResult};
use crate::mm::pmm::{BitmapFrameAllocator, FRAME_ALLOCATOR};
use core::arch::asm;

// =============================================================================
// FLAGS DE PAGINA√á√ÉO x86_64
// =============================================================================
// Flags usadas neste m√≥dulo. Consulte Intel SDM para significado detalhado.

/// M√°scara para extrair endere√ßo f√≠sico de uma PTE (bits 12-51)
pub const PAGE_MASK: u64 = 0x000F_FFFF_FFFF_F000;

/// P√°gina presente na mem√≥ria f√≠sica.
pub const PAGE_PRESENT: u64 = 1 << 0;

/// P√°gina pode ser escrita.
pub const PAGE_WRITABLE: u64 = 1 << 1;

/// P√°gina acess√≠vel em modo usu√°rio (Ring 3).
pub const PAGE_USER: u64 = 1 << 2;

/// Huge page (2 MiB quando definida em PD). Quando ativa, o CPU ignora PT.
pub const PAGE_HUGE: u64 = 1 << 7;

/// Disable Execute (NX) ‚Äî marca p√°gina como n√£o execut√°vel.
pub const PAGE_NO_EXEC: u64 = 1 << 63;

// =============================================================================
// ESTADO GLOBAL DO VMM
// =============================================================================

/// Endere√ßo f√≠sico da PML4 ativa (valor extra√≠do do CR3 no `init()`).
static mut ACTIVE_PML4_PHYS: u64 = 0;

// =============================================================================
// SCRATCH SLOT ‚Äî mecanismo para zeragem segura de frames
// =============================================================================
//
// O scratch slot √© uma pe√ßa cr√≠tica do early-boot: fornece um endere√ßo virtual
// limpo (garantido n√£o coberto por huge pages) onde podemos mapear um frame f√≠sico
// temporariamente para opera√ß√µes primitivas (zerar, copiar).
//
// - O bootloader deve reservar esse √≠ndice PML4 e criar a estrutura PML4‚ÜíPDPT‚ÜíPD‚ÜíPT
//   para o scratch slot antes de transferir controle ao kernel.
// - Aqui validamos e usamos essa PT; se n√£o existir, o m√≥dulo registra aviso e
//   tenta fallback (menos seguro).
//
// Observa√ß√£o: se voc√™ mudar o √≠ndice do scratch, atualize o bootloader (ignite).
//
const SCRATCH_VIRT: u64 = 0xFFFF_FE00_0000_0000; // virtual address acordado para scratch
static mut SCRATCH_PT_PHYS: u64 = 0; // endere√ßo f√≠sico da PT do scratch
static mut SCRATCH_READY: bool = false; // indica disponibilidade operacional

// =============================================================================
// INICIALIZA√á√ÉO DO VMM
// =============================================================================

/// Inicializa o VMM a partir do contexto fornecido pelo bootloader.
///
/// Pr√©-condi√ß√µes:
/// - O bootloader j√° estabeleceu identity map (0..4GiB) com huge pages.
/// - O bootloader criou a hierarquia do scratch slot no PML4 acordado.
///
/// Safety:
/// - Deve ser invocado **uma √∫nica vez** em early-boot.
/// - O caller deve garantir que o ambiente (CR3, boot_info) esteja consistente.
pub unsafe fn init(boot_info: &crate::core::handoff::BootInfo) {
    crate::kdebug!("(VMM) init: Iniciando...");

    // L√™ CR3 (PML4 f√≠sico atual) e guarda para uso interno.
    let cr3: u64;
    asm!("mov {}, cr3", out(reg) cr3);
    ACTIVE_PML4_PHYS = cr3 & 0x000F_FFFF_FFFF_F000;

    crate::kdebug!(
        "(VMM) init: CR3={:#x}, PML4_PHYS={:#x}",
        cr3,
        ACTIVE_PML4_PHYS
    );
    crate::ktrace!("(VMM) init: SCRATCH_VIRT={:#x}", SCRATCH_VIRT);

    // Valida e inicializa o scratch slot para opera√ß√µes de zeragem.
    init_scratch_slot();

    if SCRATCH_READY {
        crate::kdebug!(
            "(VMM) init: Scratch slot pronto em PT {:#x}",
            SCRATCH_PT_PHYS
        );
    } else {
        crate::kwarn!("(VMM) init: Scratch slot N√ÉO dispon√≠vel - usando fallback");
    }

    // (nota) O `boot_info` pode ser usado para logging adicional
    let _ = boot_info;
    crate::kdebug!("(VMM) init: OK");
}

/// Localiza e valida a Page Table do scratch slot criada pelo bootloader.
///
/// Esta fun√ß√£o percorre PML4‚ÜíPDPT‚ÜíPD‚ÜíPT e verifica que:
/// - entradas est√£o marcadas PRESENT;
/// - a entrada do PD n√£o √© uma HUGE PAGE (o que invalidaria a PT).
///
/// Se qualquer verifica√ß√£o falhar, a fun√ß√£o registra o problema e marca
/// `SCRATCH_READY = false` (fallback ou corre√ß√£o manual necess√°ria).
unsafe fn init_scratch_slot() {
    crate::kdebug!("(VMM) Inicializando scratch slot...");

    let pml4_idx = ((SCRATCH_VIRT >> 39) & 0x1FF) as usize;
    let pdpt_idx = ((SCRATCH_VIRT >> 30) & 0x1FF) as usize;
    let pd_idx = ((SCRATCH_VIRT >> 21) & 0x1FF) as usize;

    // Usar phys_to_virt para acessar PML4
    let pml4: *const u64 = phys_to_virt(ACTIVE_PML4_PHYS);
    let pml4_entry = *pml4.add(pml4_idx);

    if pml4_entry & PAGE_PRESENT == 0 {
        crate::kwarn!("(VMM) Scratch: PML4[{}] n√£o presente", pml4_idx);
        SCRATCH_READY = false;
        return;
    }

    let pdpt_phys = pml4_entry & PAGE_MASK;
    let pdpt: *const u64 = phys_to_virt(pdpt_phys);
    let pdpt_entry = *pdpt.add(pdpt_idx);

    if pdpt_entry & PAGE_PRESENT == 0 {
        crate::kwarn!("(VMM) Scratch: PDPT[{}] n√£o presente", pdpt_idx);
        SCRATCH_READY = false;
        return;
    }

    let pd_phys = pdpt_entry & PAGE_MASK;
    let pd: *const u64 = phys_to_virt(pd_phys);
    let pd_entry = *pd.add(pd_idx);

    if pd_entry & PAGE_PRESENT == 0 {
        crate::kwarn!("(VMM) Scratch: PD[{}] n√£o presente", pd_idx);
        SCRATCH_READY = false;
        return;
    }

    if pd_entry & PAGE_HUGE != 0 {
        crate::kwarn!("(VMM) Scratch: PD[{}] √© huge page!", pd_idx);
        SCRATCH_READY = false;
        return;
    }

    SCRATCH_PT_PHYS = pd_entry & PAGE_MASK;
    SCRATCH_READY = true;
    crate::kdebug!("(VMM) Scratch slot OK: PT em {:#x}", SCRATCH_PT_PHYS);
}

// =============================================================================
// ZERAGEM DE FRAMES F√çSICOS (USANDO SCRATCH OU IDENTITY MAP)
// =============================================================================

/// Zera um frame f√≠sico com seguran√ßa.
///
/// # Estrat√©gia
///
/// 1. Se SCRATCH_READY: usa scratch slot (mais seguro, funciona para qualquer endere√ßo)
/// 2. Se n√£o: usa phys_to_virt via identity map (s√≥ funciona para phys < 4GB)
///
/// # Returns
///
/// - `Ok(())` se o frame foi zerado com sucesso
/// - `Err(MmError::ScratchNotReady)` se scratch indispon√≠vel e phys >= 4GB
unsafe fn zero_frame_via_scratch(phys: u64) -> MmResult<()> {
    // Usar scratch slot se dispon√≠vel (m√©todo preferido)
    if SCRATCH_READY {
        let pt_idx = ((SCRATCH_VIRT >> 12) & 0x1FF) as usize;
        let pt_ptr: *mut u64 = phys_to_virt(SCRATCH_PT_PHYS);
        let pte_ptr = pt_ptr.add(pt_idx);

        // Salvar PTE original
        let original_pte = core::ptr::read_volatile(pte_ptr);

        // Mapear frame no scratch slot
        let temp_pte = (phys & PAGE_MASK) | PAGE_PRESENT | PAGE_WRITABLE;
        core::ptr::write_volatile(pte_ptr, temp_pte);
        asm!("invlpg [{}]", in(reg) SCRATCH_VIRT, options(nostack, preserves_flags));

        // Zerar via endere√ßo virtual do scratch usando while + assembly
        let base = SCRATCH_VIRT as *mut u64;
        let mut i = 0usize;
        let zero_val = 0u64;
        while i < 512 {
            let ptr = base.add(i);
            core::arch::asm!(
                "mov [{0}], {1}",
                in(reg) ptr,
                in(reg) zero_val,
                options(nostack, preserves_flags)
            );
            i += 1;
        }

        // Restaurar PTE original
        core::ptr::write_volatile(pte_ptr, original_pte);
        asm!("invlpg [{}]", in(reg) SCRATCH_VIRT, options(nostack, preserves_flags));

        return Ok(());
    }

    // Fallback: usar identity map via phys_to_virt (s√≥ funciona para < 4GB)
    if !crate::mm::addr::is_phys_accessible(phys) {
        crate::kerror!(
            "(VMM) zero_frame: phys {:#x} inacess√≠vel sem scratch!",
            phys
        );
        return Err(MmError::ScratchNotReady);
    }

    crate::ktrace!("(VMM) zero_frame: usando identity map para {:#x}", phys);
    let ptr: *mut u64 = phys_to_virt(phys);
    let mut i = 0usize;
    let zero_val = 0u64;
    while i < 512 {
        let entry_ptr = ptr.add(i);
        core::arch::asm!(
            "mov [{0}], {1}",
            in(reg) entry_ptr,
            in(reg) zero_val,
            options(nostack, preserves_flags)
        );
        i += 1;
    }

    Ok(())
}

// =============================================================================
// MAPEAMENTO DE P√ÅGINAS VIRTUAIS (APIs P√öBLICAS)
// =============================================================================

/// Mapeia `virt_addr` ‚Üí `phys_addr` com `flags`.
///
/// - Esta vers√£o adquire o lock global do FRAME_ALLOCATOR internamente.
/// - Para evitar deadlocks em caminhos que j√° t√™m o lock do PMM, use
///   `map_page_with_pmm()` passando o PMM explicitamente.
///
/// Retorna `true` em sucesso, `false` em OOM ou erro.
pub unsafe fn map_page(virt_addr: u64, phys_addr: u64, flags: u64) -> bool {
    let mut pmm = FRAME_ALLOCATOR.lock();
    map_page_with_pmm(virt_addr, phys_addr, flags, &mut *pmm)
}

/// Mapeia `virt_addr` ‚Üí `phys_addr` usando o PMM fornecido.
///
/// Algoritmo:
/// 1. Calcula √≠ndices PML4/PDPT/PD/PT a partir de `virt_addr`.
/// 2. Para cada n√≠vel, garante que exista uma tabela (alocando via PMM se necess√°rio).
/// 3. Se encontrar uma entrada PD marcada como HUGE, faz SPLIT para PT (512 entradas).
/// 4. Escreve a PTE final e invalida TLB.
///
/// Use esta fun√ß√£o quando voc√™ j√° segurou o lock do PMM (evita deadlock).
pub unsafe fn map_page_with_pmm(
    virt_addr: u64,
    phys_addr: u64,
    flags: u64,
    pmm: &mut BitmapFrameAllocator,
) -> bool {
    // DEBUG: Log de entrada apenas na primeira p√°gina
    static mut FIRST_MAP: bool = true;
    let is_first = FIRST_MAP;
    if is_first {
        FIRST_MAP = false;
        crate::ktrace!(
            "(VMM) map_page_with_pmm: virt={:#x} phys={:#x}",
            virt_addr,
            phys_addr
        );
    }

    // √çndices da hierarquia (9 bits cada)
    let pml4_idx = ((virt_addr >> 39) & 0x1FF) as usize;
    let pdpt_idx = ((virt_addr >> 30) & 0x1FF) as usize;
    let pd_idx = ((virt_addr >> 21) & 0x1FF) as usize;
    let pt_idx = ((virt_addr >> 12) & 0x1FF) as usize;

    if is_first {
        crate::ktrace!(
            "(VMM) indices: pml4={} pdpt={} pd={} pt={}",
            pml4_idx,
            pdpt_idx,
            pd_idx,
            pt_idx
        );
    }

    // Ponteiro para a PML4 atual via phys_to_virt
    let pml4_ptr: *mut u64 = phys_to_virt(ACTIVE_PML4_PHYS);

    if is_first {
        crate::ktrace!("(VMM) pml4_ptr = {:p}", pml4_ptr);
    }

    let pml4_entry = &mut *pml4_ptr.add(pml4_idx);

    if is_first {
        crate::ktrace!(
            "(VMM) pml4_entry = {:#x}, chamando ensure_table...",
            *pml4_entry
        );
    }

    // Garantir PDPT existe
    let pdpt_phys = ensure_table_entry_with_pmm(pml4_entry, pmm);
    if pdpt_phys == 0 {
        crate::kerror!("(VMM) map_page: falha ao criar PDPT para {:#x}", virt_addr);
        return false;
    }

    let pdpt_ptr: *mut u64 = phys_to_virt(pdpt_phys);
    let pdpt_entry = &mut *pdpt_ptr.add(pdpt_idx);

    // Garantir PD existe
    let pd_phys = ensure_table_entry_with_pmm(pdpt_entry, pmm);
    if pd_phys == 0 {
        crate::kerror!("(VMM) map_page: falha ao criar PD para {:#x}", virt_addr);
        return false;
    }

    let pd_ptr: *mut u64 = phys_to_virt(pd_phys);
    let pd_entry = &mut *pd_ptr.add(pd_idx);

    // Garantir PT existe (pode exigir split de huge page)
    let pt_phys = ensure_table_entry_with_pmm(pd_entry, pmm);
    if pt_phys == 0 {
        crate::kerror!("(VMM) map_page: falha ao criar PT para {:#x}", virt_addr);
        return false;
    }

    let pt_ptr: *mut u64 = phys_to_virt(pt_phys);
    let pt_entry = &mut *pt_ptr.add(pt_idx);

    // Escrever PTE final (endere√ßo f√≠sico + flags + PRESENT)
    *pt_entry = (phys_addr & PAGE_MASK) | flags | PAGE_PRESENT;

    // Invalida TLB para o endere√ßo mapeado
    asm!("invlpg [{}]", in(reg) virt_addr, options(nostack, preserves_flags));

    true
}

// =============================================================================
// GERENCIAMENTO DE ENTRADAS DE PAGE TABLES (CORE)
// =============================================================================

/// Garante que a entrada de uma page-table (`entry`) aponte para uma tabela v√°lida.
///
/// Comportamento:
/// - Se `entry` j√° estiver presente e n√£o for HUGE, retorna o endere√ßo da tabela.
/// - Se `entry` for HUGE, realiza SPLIT: aloca uma PT e replica o mapeamento da huge page
///   em 512 entradas de 4 KiB, atualizando a entrada para apontar para a nova PT.
/// - Se `entry` n√£o estiver presente, aloca uma nova tabela (frame), zera-a e retorna seu endere√ßo.
///
/// Retorna o endere√ßo f√≠sico da tabela (PDPT/PD/PT conforme o n√≠vel), ou 0 em caso de OOM.
///
/// Safety:
/// - `entry` deve ser um ponteiro v√°lido para uma entrada da page table.
/// - `pmm` deve estar inicializado e operante.
unsafe fn ensure_table_entry_with_pmm(entry: &mut u64, pmm: &mut BitmapFrameAllocator) -> u64 {
    // Caso: entrada j√° existe
    if *entry & PAGE_PRESENT != 0 {
        // Se for uma huge page, precisamos converter para uma PT de 4 KiB (split)
        if *entry & PAGE_HUGE != 0 {
            // --- SPLIT DE HUGE PAGE ---
            crate::kdebug!("(VMM) Splitting huge page...");

            // Extra√≠mos a base f√≠sica da huge page (alinhada a 2 MiB).
            let huge_base = *entry & 0x000F_FFFF_FFE0_0000;

            // Preservamos flags relevantes, exceto o bit HUGE.
            let old_flags = *entry & 0xFFF;

            // Aloca frame para a nova PT (4 KiB)
            let frame = match pmm.allocate_frame() {
                Some(f) => f,
                None => {
                    crate::kerror!("(VMM) OOM ao fazer split de huge page!");
                    return 0;
                }
            };
            let pt_phys = frame.addr;

            // Zera a p√°gina que conter√° a nova PT antes de escrever entradas.
            if let Err(e) = zero_frame_via_scratch(pt_phys) {
                crate::kerror!("(VMM) Falha ao zerar PT para split: {}", e);
                return 0;
            }

            // Preenche a PT replicando o mapeamento da huge page em 512 entradas.
            let pt: *mut u64 = phys_to_virt(pt_phys);
            let new_flags = (old_flags & !PAGE_HUGE) | PAGE_PRESENT | PAGE_WRITABLE;

            let mut j = 0usize;
            while j < 512 {
                let page_phys = huge_base + (j as u64 * 4096);
                let entry_val = page_phys | new_flags;
                let entry_ptr = pt.add(j);
                core::arch::asm!(
                    "mov [{0}], {1}",
                    in(reg) entry_ptr,
                    in(reg) entry_val,
                    options(nostack, preserves_flags)
                );
                j += 1;
            }

            // Atualiza a entrada do PD para apontar para a nova PT (removendo HUGE).
            *entry = pt_phys | PAGE_PRESENT | PAGE_WRITABLE;

            // Invalidar TLB para toda a regi√£o anteriormente mapeada como huge page.
            let mut k = 0u64;
            while k < 512 {
                let vaddr = huge_base + (k * 4096);
                asm!("invlpg [{}]", in(reg) vaddr, options(nostack, preserves_flags));
                k += 1;
            }

            crate::kdebug!("(VMM) Huge page split OK: nova PT em {:#x}", pt_phys);
            return pt_phys;
        }

        // Entrada presente e n√£o-huge: garantir flags de acesso e retornar endere√ßo.
        *entry |= PAGE_USER | PAGE_WRITABLE;
        return *entry & PAGE_MASK;
    }

    // Caso: entrada ausente ‚Äî aloca nova tabela (frame de 4 KiB)
    let frame = match pmm.allocate_frame() {
        Some(f) => f,
        None => {
            crate::kerror!("(VMM) OOM ao alocar page table!");
            return 0;
        }
    };
    let phys = frame.addr;

    // Zera a nova tabela (cr√≠tico para evitar leitura de lixo).
    if let Err(e) = zero_frame_via_scratch(phys) {
        crate::kerror!("(VMM) Falha ao zerar nova page table: {}", e);
        return 0;
    }

    // Configura a entrada para apontar para a nova tabela: PRESENT | WRITABLE | USER.
    *entry = phys | PAGE_PRESENT | PAGE_WRITABLE | PAGE_USER;

    phys
}

/// Traduz um endere√ßo virtual para f√≠sico.
/// Retorna None se n√£o estiver mapeado.
pub fn translate_addr(virt_addr: u64) -> Option<u64> {
    unsafe {
        let pml4_idx = ((virt_addr >> 39) & 0x1FF) as usize;
        let pml4_ptr: *const u64 = phys_to_virt(ACTIVE_PML4_PHYS);
        let pml4_entry = *pml4_ptr.add(pml4_idx);
        if pml4_entry & PAGE_PRESENT == 0 {
            return None;
        }

        let pdpt_phys = pml4_entry & PAGE_MASK;
        let pdpt_ptr: *const u64 = phys_to_virt(pdpt_phys);
        let pdpt_idx = ((virt_addr >> 30) & 0x1FF) as usize;
        let pdpt_entry = *pdpt_ptr.add(pdpt_idx);
        if pdpt_entry & PAGE_PRESENT == 0 {
            return None;
        }

        // Huge page (1GB) check
        if pdpt_entry & PAGE_HUGE != 0 {
            let base = pdpt_entry & 0x000F_FFFF_C000_0000;
            let offset = virt_addr & 0x3FFF_FFFF;
            return Some(base + offset);
        }

        let pd_phys = pdpt_entry & PAGE_MASK;
        let pd_ptr: *const u64 = phys_to_virt(pd_phys);
        let pd_idx = ((virt_addr >> 21) & 0x1FF) as usize;
        let pd_entry = *pd_ptr.add(pd_idx);
        if pd_entry & PAGE_PRESENT == 0 {
            return None;
        }

        // Huge page (2MB) check
        if pd_entry & PAGE_HUGE != 0 {
            let base = pd_entry & 0x000F_FFFF_FFE0_0000;
            let offset = virt_addr & 0x1FFFFF;
            return Some(base + offset);
        }

        let pt_phys = pd_entry & PAGE_MASK;
        let pt_ptr: *const u64 = phys_to_virt(pt_phys);
        let pt_idx = ((virt_addr >> 12) & 0x1FF) as usize;
        let pt_entry = *pt_ptr.add(pt_idx);
        if pt_entry & PAGE_PRESENT == 0 {
            return None;
        }

        let phys_base = pt_entry & PAGE_MASK;
        Some(phys_base + (virt_addr & 0xFFF))
    }
}

/// Traduz endere√ßo virtual para f√≠sico e retorna as flags da p√°gina.
pub fn translate_addr_with_flags(virt_addr: u64) -> Option<(u64, u64)> {
    unsafe {
        let pml4_idx = ((virt_addr >> 39) & 0x1FF) as usize;
        let pml4_ptr: *const u64 = phys_to_virt(ACTIVE_PML4_PHYS);
        let pml4_entry = *pml4_ptr.add(pml4_idx);
        if pml4_entry & PAGE_PRESENT == 0 {
            return None;
        }

        let pdpt_phys = pml4_entry & PAGE_MASK;
        let pdpt_ptr: *const u64 = phys_to_virt(pdpt_phys);
        let pdpt_idx = ((virt_addr >> 30) & 0x1FF) as usize;
        let pdpt_entry = *pdpt_ptr.add(pdpt_idx);
        if pdpt_entry & PAGE_PRESENT == 0 {
            return None;
        }

        if pdpt_entry & PAGE_HUGE != 0 {
            let base = pdpt_entry & 0x000F_FFFF_C000_0000;
            let offset = virt_addr & 0x3FFF_FFFF;
            return Some((base + offset, pdpt_entry & 0xFFF));
        }

        let pd_phys = pdpt_entry & PAGE_MASK;
        let pd_ptr: *const u64 = phys_to_virt(pd_phys);
        let pd_idx = ((virt_addr >> 21) & 0x1FF) as usize;
        let pd_entry = *pd_ptr.add(pd_idx);
        if pd_entry & PAGE_PRESENT == 0 {
            return None;
        }

        if pd_entry & PAGE_HUGE != 0 {
            let base = pd_entry & 0x000F_FFFF_FFE0_0000;
            let offset = virt_addr & 0x1FFFFF;
            return Some((base + offset, pd_entry & 0xFFF));
        }

        let pt_phys = pd_entry & PAGE_MASK;
        let pt_ptr: *const u64 = phys_to_virt(pt_phys);
        let pt_idx = ((virt_addr >> 12) & 0x1FF) as usize;
        let pt_entry = *pt_ptr.add(pt_idx);
        if pt_entry & PAGE_PRESENT == 0 {
            return None;
        }

        let phys_base = pt_entry & PAGE_MASK;
        Some((phys_base + (virt_addr & 0xFFF), pt_entry & 0xFFF))
    }
}
